{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, warnings\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "from glob import glob1\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "from utils import complex_to_polar, polar_to_complex\n",
    "\n",
    "class Train_Generator(Sequence) : \n",
    "    def __init__(self, src_path, bulk_num=5, sample_dur=5, max_cache_size=10000, n_fft=1918, win_length=1024, shuffle=False) :\n",
    "        \n",
    "        def sort_via_dur(input_list) : \n",
    "            input_list = sorted(input_list)\n",
    "            input_list = [[path, librosa.get_duration(path=path)] for path in input_list]\n",
    "            input_list = sorted(input_list, key=lambda x : x[1], reverse=True)\n",
    "            return [var[0] for var in input_list]\n",
    "        \n",
    "        self._input_list = [src_path + name for name in glob1(dirname=src_path, pattern=\"merge*\")]\n",
    "        self._output_list1 = [src_path + name for name in glob1(dirname=src_path, pattern=\"voice*\")]\n",
    "        self._output_list2 = [src_path + name for name in glob1(dirname=src_path, pattern=\"music*\")]\n",
    "        \n",
    "        num_input = len(self._input_list)\n",
    "        num_output1 = len(self._output_list1)\n",
    "        num_output2 = len(self._output_list2)\n",
    "        if not ((num_input == num_output1) and (num_output1 == num_output2)) : \n",
    "            raise AssertionError(\"The number of source sample must be same. : [{}, {}, {}]\".format(num_input, num_output1, num_output2))\n",
    "        \n",
    "        self._input_list = sort_via_dur(self._input_list)\n",
    "        self._output_list1 = sort_via_dur(self._output_list1)\n",
    "        self._output_list2 = sort_via_dur(self._output_list2)\n",
    "\n",
    "        self._bulk_num = bulk_num\n",
    "        self._sample_dur = sample_dur\n",
    "        self._max_cache_size = max_cache_size\n",
    "        self._n_fft = n_fft\n",
    "        self._win_length = win_length\n",
    "\n",
    "        self._src_index = 0\n",
    "        self._duration_list = np.zeros_like(self._input_list, dtype=np.float32)\n",
    "        self._src_index_list = [i for i in range(len(self._input_list))]\n",
    "        if shuffle : np.random.shuffle(self.src_index_list)\n",
    "\n",
    "        self._max_dur = self.__estimate_max_len(self._src_index)\n",
    "        self.sample_x, self.sample_y1, self.sample_y2 = self.__get_sample()\n",
    "\n",
    "        self.__cache_warning()\n",
    "    \n",
    "    def __get_sample(self) : \n",
    "        X, (Y1, Y2) = self.__load_data(self._src_index, update=False)\n",
    "        return X, Y1, Y2\n",
    "\n",
    "    def __src_index_list(self, src_index) : \n",
    "        return [i % len(self._input_list) for i in range(src_index, src_index + self._bulk_num)]\n",
    "    \n",
    "    def __resource_validation(self, X : ndarray, Y1 : ndarray, Y2 : ndarray) : \n",
    "        assert (X.shape == Y1.shape) and (Y1.shape == Y2.shape) \\\n",
    "            , ValueError(\"The shape of IO is different : [{} / {}, {}]\".format(X.shape, Y1.shape, Y2.shape))\n",
    "\n",
    "    def __load_single_data(self, src_index, path_list, update=True) -> ndarray : \n",
    "        path = path_list[src_index]\n",
    "        sample_rate = librosa.get_samplerate(path)\n",
    "        source_duration = librosa.get_duration(path=path) - 0.1\n",
    "\n",
    "        if update : \n",
    "            if self._duration_list[src_index] + self._sample_dur >= source_duration : \n",
    "                source_offset = source_duration - self._sample_dur\n",
    "                self._duration_list[src_index] = 0\n",
    "            else : \n",
    "                source_offset = self._duration_list[src_index]\n",
    "                self._duration_list[src_index] += self._sample_dur\n",
    "        else : \n",
    "            source_offset = self._duration_list[src_index]\n",
    "        \n",
    "        data_source = librosa.load(path=path, sr=sample_rate, offset=source_offset, duration=self._sample_dur)[0]\n",
    "        D = librosa.stft(data_source, n_fft=self._n_fft, win_length=self._win_length)\n",
    "        del data_source\n",
    "        return D\n",
    "    \n",
    "    def __load_data(self, src_index, update=True) : \n",
    "        src_index_list = self.__src_index_list(src_index)\n",
    "        \n",
    "        input_list = []\n",
    "        output1_list = []\n",
    "        output2_list = []\n",
    "\n",
    "        for index in src_index_list : \n",
    "            X = self.__load_single_data(index, self._input_list, update)\n",
    "            Y1 = self.__load_single_data(index, self._output_list1, update)\n",
    "            Y2 = self.__load_single_data(index, self._output_list2, update)\n",
    "\n",
    "            quotient_x = len(X[0]) // 64\n",
    "            quotient_y1 = len(Y1[0]) // 64\n",
    "            quotient_y2 = len(Y2[0]) // 64\n",
    "            X = X[:,:quotient_x * 64]\n",
    "            Y1 = Y1[:,:quotient_y1 * 64]\n",
    "            Y2 = Y2[:,:quotient_y2 * 64]\n",
    "            self.__resource_validation(X, Y1, Y2)\n",
    "\n",
    "            input_list.append(X)\n",
    "            output1_list.append(Y1)\n",
    "            output2_list.append(Y2)\n",
    "\n",
    "        data_input = complex_to_polar(np.array(input_list))\n",
    "        data_output1 = complex_to_polar(np.array(output1_list))\n",
    "        data_output2 = complex_to_polar(np.array(output2_list))\n",
    "\n",
    "        del input_list, output1_list, output2_list\n",
    "\n",
    "        return data_input, (data_output1, data_output2)\n",
    "\n",
    "    def __estimate_max_len(self, src_index) : \n",
    "        src_index_list = self.__src_index_list(src_index)\n",
    "        max = 0\n",
    "        for index in src_index_list : \n",
    "            input_path = self._input_list[index]\n",
    "            dur = librosa.get_duration(path=input_path)\n",
    "            if max < dur : max = dur\n",
    "        return max\n",
    "\n",
    "    def __estimate_cache(self) : \n",
    "        current_cache = 0\n",
    "        for sample in [self.sample_x, self.sample_y1, self.sample_y2] : \n",
    "            current_cache += getsizeof(sample)\n",
    "        return current_cache\n",
    "\n",
    "    def __cache_warning(self) : \n",
    "        if self._max_cache_size <= self.__estimate_cache() : \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"always\")\n",
    "                warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n",
    "\n",
    "    def __len__(self) :\n",
    "        return int(self._max_dur // self._sample_dur)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.__load_data(self._src_index)\n",
    "    \n",
    "    def on_epoch_end(self) :\n",
    "        del self.sample_x, self.sample_y1, self.sample_y2\n",
    "\n",
    "        self._src_index += self._bulk_num\n",
    "        self._src_index %= len(self._input_list)\n",
    "        self.sample_x, self.sample_y1, self.sample_y2 = self.__get_sample()\n",
    "        self.__cache_warning()\n",
    "    \n",
    "    @property\n",
    "    def cache_size(self) : \n",
    "        return self.__estimate_cache()\n",
    "\n",
    "    @property\n",
    "    def input_shape(self) : \n",
    "        return self.sample_x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... [-] : [002/006]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n",
      "[src/libmpg123/id3.c:process_comment():584] error: No comment text / valid description?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... [-] : [006/006]\t  Done\n"
     ]
    }
   ],
   "source": [
    "from utils import gen_dataset\n",
    "\n",
    "gen_dataset(target_dir=\"./test_sample/\", train_test_split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "def calc_mem_usage(x) : \n",
    "    mem_use = x if type(x) == int else getsizeof(x)\n",
    "    unit_list = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    count = 0\n",
    "    while mem_use >= 1000 : \n",
    "        mem_use /= 1024\n",
    "        count += 1\n",
    "    \n",
    "    if count >= len(unit_list) : \n",
    "        raise AssertionError(\"Memory usage is out of 1024 TB\")\n",
    "\n",
    "    else : \n",
    "        print(\"{:,.3f} {}ytes\".format(mem_use, unit_list[count]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 832, 2)\n",
      "(3, 960, 832, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/qm8xzs4514n13h6cmcqlx7h40000gn/T/ipykernel_49475/1710470943.py:133: ResourceWarning: Loaded data exceeded max_cache_size.\n",
      "  warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n"
     ]
    }
   ],
   "source": [
    "train_path = \"./test_sample/train_data/\"\n",
    "\n",
    "train_generator = Train_Generator(train_path, bulk_num=3)\n",
    "print(train_generator.input_shape)\n",
    "print(train_generator.sample_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.844 MBytes\n"
     ]
    }
   ],
   "source": [
    "calc_mem_usage(train_generator.cache_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import gen_separate_unet\n",
    "\n",
    "test_model = gen_separate_unet(input_shape=train_generator.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 18:00:16.260352: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 93s 2s/step - loss: 1.7877 - vocal_separation_loss: 0.7763 - separation_1_loss: 1.0114\n",
      "0\n",
      "1\n",
      "\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/qm8xzs4514n13h6cmcqlx7h40000gn/T/ipykernel_49475/1710470943.py:133: ResourceWarning: Loaded data exceeded max_cache_size.\n",
      "  warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 91s 2s/step - loss: 1.7459 - vocal_separation_loss: 0.7755 - separation_1_loss: 0.9704\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/qm8xzs4514n13h6cmcqlx7h40000gn/T/ipykernel_49475/1710470943.py:133: ResourceWarning: Loaded data exceeded max_cache_size.\n",
      "  warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 91s 2s/step - loss: 1.7765 - vocal_separation_loss: 0.7739 - separation_1_loss: 1.0026\n",
      "0\n",
      "1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/qm8xzs4514n13h6cmcqlx7h40000gn/T/ipykernel_49475/1710470943.py:133: ResourceWarning: Loaded data exceeded max_cache_size.\n",
      "  warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29ed08850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "test_model.fit(x=train_generator, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
