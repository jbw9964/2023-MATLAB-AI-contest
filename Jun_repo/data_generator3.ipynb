{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"gen_separate_unet\"]\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import InputLayer, Conv2D, Dropout, BatchNormalization\n",
    "from keras.layers import Concatenate, Conv2DTranspose, LeakyReLU, ReLU\n",
    "from keras.layers import Multiply\n",
    "\n",
    "def gen_unet(input_shape=(960, 832, 2), encode=5) : \n",
    "    input_layer = InputLayer(input_shape=input_shape).input\n",
    "    \n",
    "    last_layer = input_layer\n",
    "    concate_list = []\n",
    "    filter_num = 16 * input_shape[2]                # set initial number of filters as 16\n",
    "    \n",
    "    for _ in range(encode) : \n",
    "        conv_encoder = Conv2D(filters=filter_num, kernel_size=5, strides=2, padding=\"same\")(last_layer)\n",
    "        batch_norm = BatchNormalization()(conv_encoder)\n",
    "        activ_layer = LeakyReLU(alpha=0.2)(batch_norm)\n",
    "        \n",
    "        concate_list.insert(0, conv_encoder)\n",
    "        last_layer = activ_layer\n",
    "        filter_num *= 2\n",
    "    \n",
    "    conv_layer = Conv2D(filters=filter_num, kernel_size=5, strides=2, padding=\"same\")(last_layer)\n",
    "    activ_layer = LeakyReLU(alpha=0.2)(conv_layer)\n",
    "    last_layer = activ_layer\n",
    "\n",
    "    count = 1\n",
    "    for concate_layer in concate_list : \n",
    "        filter_num /= 2\n",
    "\n",
    "        conv_trans = Conv2DTranspose(filters=filter_num, kernel_size=5, strides=2, padding=\"same\")(last_layer)\n",
    "        merge_layer = Concatenate(axis=3)([conv_trans, concate_layer])\n",
    "        conv_decoder = Conv2D(filters=filter_num, kernel_size=5, strides=1, padding=\"same\")(merge_layer)\n",
    "        batch_norm = BatchNormalization()(conv_decoder)\n",
    "        activ_layer = ReLU()(batch_norm)\n",
    "\n",
    "        if count <= 3 : \n",
    "            last_layer = Dropout(rate=0.5)(activ_layer)\n",
    "        \n",
    "        else : \n",
    "            last_layer = activ_layer\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    conv_layer = Conv2DTranspose(filters=8 * input_shape[2], kernel_size=5, strides=2, padding=\"same\")(last_layer)\n",
    "    conv_layer = Conv2DTranspose(filters=input_shape[2], kernel_size=3, strides=1, padding=\"same\", activation=\"sigmoid\")(conv_layer)\n",
    "    output_layer = Multiply()([input_layer, conv_layer])\n",
    "    \n",
    "    return Model(inputs=[input_layer], outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, warnings\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "from glob import glob1\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from numpy import ndarray\n",
    "\n",
    "from utils import complex_to_polar\n",
    "\n",
    "class TrainGenerator(Sequence) : \n",
    "    def __init__(\n",
    "            self, src_path : str, bulk_num : int=5, \n",
    "            sample_dur : float=5, max_cache_size : float=2, restrict_cache=False,\n",
    "            n_fft : int=1918, win_length : int=1024, \n",
    "            sample_rate=None, shuffle=True\n",
    "    ) :\n",
    "        if src_path[-1] != \"/\" : src_path += \"/\"\n",
    "\n",
    "        input_list = [src_path + name for name in glob1(dirname=src_path, pattern=\"merge*\")]\n",
    "        output_list = [src_path + name for name in glob1(dirname=src_path, pattern=\"voice*\")]\n",
    "        \n",
    "        assert len(input_list) == len(output_list)\\\n",
    "            , AssertionError(\"The number of source sample must be same. : [{}, | {}]\".format(\n",
    "            len(input_list), len(output_list)\n",
    "            ))\n",
    "        assert len(input_list), AssertionError(\"In src_path, no match with pattern [merge*]\")\n",
    "        assert len(output_list), AssertionError(\"In src_path, no match with pattern [voice*]\")\n",
    "        \n",
    "        def sort_via_dur(input_list) : \n",
    "            var_list = sorted(input_list)\n",
    "            var_list = [[path, librosa.get_duration(path=path)] for path in var_list]\n",
    "            var_list = sorted(var_list, key=lambda x : x[1], reverse=True)\n",
    "            return [var[0] for var in var_list]\n",
    "        \n",
    "        self._input_path = sort_via_dur(input_list)\n",
    "        self._output_path = sort_via_dur(output_list)\n",
    "\n",
    "        self._bulk_num = bulk_num\n",
    "        self._sample_rate = sample_rate\n",
    "        self._sample_dur = sample_dur\n",
    "        \n",
    "        self._offset_list = np.zeros_like(self._input_path, dtype=np.float32)\n",
    "        self._max_dur_list = [librosa.get_duration(path=path) for path in self._input_path]\n",
    "\n",
    "        self._index_list = [i for i in range(len(self._input_path))]\n",
    "        if shuffle : np.random.shuffle(self._index_list)\n",
    "        \n",
    "        self._n_fft = n_fft\n",
    "        self._win_len = win_length\n",
    "\n",
    "        rate = librosa.get_samplerate(path=self._input_path[0]) if not sample_rate else sample_rate\n",
    "        sample_source = librosa.load(path=self._input_path[0], sr=rate, duration=sample_dur)[0]\n",
    "        self._sample_arr = librosa.stft(sample_source, n_fft=n_fft, win_length=win_length)\n",
    "        quotient = len(self._sample_arr[0]) // 64\n",
    "        self._sample_arr = complex_to_polar(self._sample_arr[:,:quotient * 64])\n",
    "\n",
    "        self._src_index = 0\n",
    "\n",
    "        self._max_cache_size = max_cache_size * (1024**3)\n",
    "        self._restrict_cache = restrict_cache\n",
    "\n",
    "        self.__cache_warning()\n",
    "    \n",
    "    def __cache_warning(self) : \n",
    "        if self._max_cache_size <= self.__estimate_cache() : \n",
    "            if self._restrict_cache : \n",
    "                raise MemoryError(\"Loaded data exceeded max_cache_size.\")\n",
    "            else : \n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"always\")\n",
    "                    warnings.warn(\"Loaded data exceeded max_cache_size.\", ResourceWarning)\n",
    "    \n",
    "    def __estimate_cache(self) : \n",
    "        return getsizeof(self._sample_arr) * self._bulk_num * 2\n",
    "\n",
    "    def __resource_validation(self, arr : ndarray) : \n",
    "        shape1, shape2 = arr.shape, self._sample_arr.shape\n",
    "        assert shape1 == shape2, AssertionError(\"The shape of IO is different : [{} | {}]\".format(shape1, shape2))\n",
    "    \n",
    "    def __get_src_list(self, src_index) : \n",
    "        return [i % len(self._input_path) for i in range(src_index, src_index + self._bulk_num)]\n",
    "\n",
    "    def __load_data(self, src_index, update=True) : \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for index in self.__get_src_list(src_index) : \n",
    "            x_list.append(self.__load_single_data(index, self._input_path))\n",
    "            y_list.append(self.__load_single_data(index, self._output_path))\n",
    "            if update : self._offset_list[index] += self._sample_dur\n",
    "        \n",
    "        return np.array(x_list), np.array(y_list)\n",
    "\n",
    "    def __load_single_data(self, src_index, path_list) : \n",
    "        path = path_list[src_index]\n",
    "        offset = self._offset_list[src_index]\n",
    "        sample_dur = self._sample_dur\n",
    "        max_dur = self._max_dur_list[src_index] - 0.1\n",
    "        \n",
    "        if offset + sample_dur >= max_dur : \n",
    "            self._offset_list[src_index] = 0\n",
    "            return self.__load_single_data(src_index, path_list)\n",
    "        \n",
    "        else : \n",
    "            sample_rate = self._sample_rate if self._sample_rate else librosa.get_samplerate(path) \n",
    "            source = librosa.load(path, sr=sample_rate, offset=offset, duration=sample_dur)[0]\n",
    "            D = librosa.stft(source, n_fft=self._n_fft, win_length=self._win_len)\n",
    "            quotient = len(D[0]) // 64\n",
    "            del source\n",
    "            D = complex_to_polar(D[:,:quotient * 64])\n",
    "            self.__resource_validation(D)\n",
    "\n",
    "            return D\n",
    "\n",
    "    def __len__(self) :\n",
    "        current_index = self.__get_src_list(self._src_index)\n",
    "        max = 0\n",
    "        for index in current_index : \n",
    "            duration = librosa.get_duration(path=self._input_path[index])\n",
    "            if duration > max : max = duration\n",
    "        \n",
    "        return int(max // self._sample_dur + 1)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.__load_data(self._src_index, update=True)\n",
    "    \n",
    "    def on_epoch_end(self) :\n",
    "        self._src_index += self._bulk_num\n",
    "        self._src_index %= len(self._input_path)\n",
    "\n",
    "        return super().on_epoch_end()\n",
    "\n",
    "    @property\n",
    "    def input_shape(self) : \n",
    "        return self._sample_arr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./test_sample/train_data/\"\n",
    "temp_gen = TrainGenerator(path, bulk_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 960, 832, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 480, 416, 32  1632        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 480, 416, 32  128        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 480, 416, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 240, 208, 64  51264       ['leaky_re_lu[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 240, 208, 64  256        ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 240, 208, 64  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 120, 104, 12  204928      ['leaky_re_lu_1[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 120, 104, 12  512        ['conv2d_2[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 120, 104, 12  0           ['batch_normalization_2[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 60, 52, 256)  819456      ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 60, 52, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 60, 52, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 30, 26, 512)  3277312     ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 30, 26, 512)  2048       ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 30, 26, 512)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 15, 13, 1024  13108224    ['leaky_re_lu_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 15, 13, 1024  0           ['conv2d_5[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 30, 26, 512)  13107712   ['leaky_re_lu_5[0][0]']          \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 30, 26, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 30, 26, 512)  13107712    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 30, 26, 512)  2048       ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 30, 26, 512)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 30, 26, 512)  0           ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 60, 52, 256)  3277056    ['dropout[0][0]']                \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 60, 52, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 60, 52, 256)  3277056     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 60, 52, 256)  1024       ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 60, 52, 256)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 60, 52, 256)  0           ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 120, 104, 12  819328     ['dropout_1[0][0]']              \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 120, 104, 25  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                6)                                'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 120, 104, 12  819328      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 120, 104, 12  512        ['conv2d_8[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 120, 104, 12  0           ['batch_normalization_7[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 120, 104, 12  0           ['re_lu_2[0][0]']                \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 240, 208, 64  204864     ['dropout_2[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 240, 208, 12  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                8)                                'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 240, 208, 64  204864      ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 240, 208, 64  256        ['conv2d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 240, 208, 64  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 480, 416, 32  51232      ['re_lu_3[0][0]']                \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 480, 416, 64  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                )                                 'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 480, 416, 32  51232       ['concatenate_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 480, 416, 32  128        ['conv2d_10[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 480, 416, 32  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 960, 832, 16  12816      ['re_lu_4[0][0]']                \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 960, 832, 2)  290        ['conv2d_transpose_5[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 960, 832, 2)  0           ['input_1[0][0]',                \n",
      "                                                                  'conv2d_transpose_6[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,404,242\n",
      "Trainable params: 52,400,274\n",
      "Non-trainable params: 3,968\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "temp_model = gen_unet(temp_gen.input_shape)\n",
    "temp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 21:34:37.904340: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 0. 0. 0.]\n",
      "[10. 10.  0.  0.  0.]\n",
      " 1/82 [..............................] - ETA: 2:27 - loss: 0.7597[15. 15.  0.  0.  0.]\n",
      " 2/82 [..............................] - ETA: 47s - loss: 0.7194 [20. 20.  0.  0.  0.]\n",
      " 3/82 [>.............................] - ETA: 47s - loss: 0.7193[25. 25.  0.  0.  0.]\n",
      " 4/82 [>.............................] - ETA: 46s - loss: 0.7084[30. 30.  0.  0.  0.]\n",
      " 5/82 [>.............................] - ETA: 45s - loss: 0.7063[35. 35.  0.  0.  0.]\n",
      " 6/82 [=>............................] - ETA: 45s - loss: 0.7122[40. 40.  0.  0.  0.]\n",
      " 7/82 [=>............................] - ETA: 44s - loss: 0.7078[45. 45.  0.  0.  0.]\n",
      " 8/82 [=>............................] - ETA: 43s - loss: 0.7034[50. 50.  0.  0.  0.]\n",
      " 9/82 [==>...........................] - ETA: 43s - loss: 0.7010[55. 55.  0.  0.  0.]\n",
      "10/82 [==>...........................] - ETA: 42s - loss: 0.6968[60. 60.  0.  0.  0.]\n",
      "11/82 [===>..........................] - ETA: 42s - loss: 0.6936[65. 65.  0.  0.  0.]\n",
      "12/82 [===>..........................] - ETA: 41s - loss: 0.6884[70. 70.  0.  0.  0.]\n",
      "13/82 [===>..........................] - ETA: 40s - loss: 0.6927[75. 75.  0.  0.  0.]\n",
      "14/82 [====>.........................] - ETA: 40s - loss: 0.7005[80. 80.  0.  0.  0.]\n",
      "15/82 [====>.........................] - ETA: 39s - loss: 0.7022[85. 85.  0.  0.  0.]\n",
      "16/82 [====>.........................] - ETA: 39s - loss: 0.7013[90. 90.  0.  0.  0.]\n",
      "17/82 [=====>........................] - ETA: 38s - loss: 0.6952[95. 95.  0.  0.  0.]\n",
      "18/82 [=====>........................] - ETA: 37s - loss: 0.6931[100. 100.   0.   0.   0.]\n",
      "19/82 [=====>........................] - ETA: 37s - loss: 0.6919[105. 105.   0.   0.   0.]\n",
      "20/82 [======>.......................] - ETA: 36s - loss: 0.6925[110. 110.   0.   0.   0.]\n",
      "21/82 [======>.......................] - ETA: 36s - loss: 0.6931[115. 115.   0.   0.   0.]\n",
      "22/82 [=======>......................] - ETA: 35s - loss: 0.6897[120. 120.   0.   0.   0.]\n",
      "23/82 [=======>......................] - ETA: 34s - loss: 0.6845[125. 125.   0.   0.   0.]\n",
      "24/82 [=======>......................] - ETA: 34s - loss: 0.6825[130. 130.   0.   0.   0.]\n",
      "25/82 [========>.....................] - ETA: 33s - loss: 0.6782[135. 135.   0.   0.   0.]\n",
      "26/82 [========>.....................] - ETA: 33s - loss: 0.6750[140. 140.   0.   0.   0.]\n",
      "27/82 [========>.....................] - ETA: 32s - loss: 0.6706[145. 145.   0.   0.   0.]\n",
      "28/82 [=========>....................] - ETA: 32s - loss: 0.6703[150. 150.   0.   0.   0.]\n",
      "29/82 [=========>....................] - ETA: 31s - loss: 0.6667[155. 155.   0.   0.   0.]\n",
      "30/82 [=========>....................] - ETA: 30s - loss: 0.6671[160. 160.   0.   0.   0.]\n",
      "31/82 [==========>...................] - ETA: 30s - loss: 0.6669[165. 165.   0.   0.   0.]\n",
      "32/82 [==========>...................] - ETA: 29s - loss: 0.6684[170. 170.   0.   0.   0.]\n",
      "33/82 [===========>..................] - ETA: 29s - loss: 0.6676[175. 175.   0.   0.   0.]\n",
      "34/82 [===========>..................] - ETA: 28s - loss: 0.6682[180. 180.   0.   0.   0.]\n",
      "35/82 [===========>..................] - ETA: 27s - loss: 0.6670[185. 185.   0.   0.   0.]\n",
      "36/82 [============>.................] - ETA: 27s - loss: 0.6710[190. 190.   0.   0.   0.]\n",
      "37/82 [============>.................] - ETA: 26s - loss: 0.6719[195. 195.   0.   0.   0.]\n",
      "38/82 [============>.................] - ETA: 26s - loss: 0.6701[200. 200.   0.   0.   0.]\n",
      "39/82 [=============>................] - ETA: 25s - loss: 0.6714[205. 205.   0.   0.   0.]\n",
      "40/82 [=============>................] - ETA: 24s - loss: 0.6741[210. 210.   0.   0.   0.]\n",
      "41/82 [==============>...............] - ETA: 24s - loss: 0.6734[215. 215.   0.   0.   0.]\n",
      "42/82 [==============>...............] - ETA: 23s - loss: 0.6744[220. 220.   0.   0.   0.]\n",
      "43/82 [==============>...............] - ETA: 23s - loss: 0.6769[225. 225.   0.   0.   0.]\n",
      "44/82 [===============>..............] - ETA: 22s - loss: 0.6795[230. 230.   0.   0.   0.]\n",
      "45/82 [===============>..............] - ETA: 21s - loss: 0.6806[235. 235.   0.   0.   0.]\n",
      "46/82 [===============>..............] - ETA: 21s - loss: 0.6781[240. 240.   0.   0.   0.]\n",
      "47/82 [================>.............] - ETA: 20s - loss: 0.6767[245. 245.   0.   0.   0.]\n",
      "48/82 [================>.............] - ETA: 20s - loss: 0.6774[250. 250.   0.   0.   0.]\n",
      "49/82 [================>.............] - ETA: 19s - loss: 0.6768[255. 255.   0.   0.   0.]\n",
      "50/82 [=================>............] - ETA: 18s - loss: 0.6742[260. 260.   0.   0.   0.]\n",
      "51/82 [=================>............] - ETA: 18s - loss: 0.6736[265. 265.   0.   0.   0.]\n",
      "52/82 [==================>...........] - ETA: 17s - loss: 0.6735[270. 270.   0.   0.   0.]\n",
      "53/82 [==================>...........] - ETA: 17s - loss: 0.6743[275. 275.   0.   0.   0.]\n",
      "54/82 [==================>...........] - ETA: 16s - loss: 0.6734[280. 280.   0.   0.   0.]\n",
      "55/82 [===================>..........] - ETA: 15s - loss: 0.6729[285. 285.   0.   0.   0.]\n",
      "56/82 [===================>..........] - ETA: 15s - loss: 0.6720[290. 290.   0.   0.   0.]\n",
      "57/82 [===================>..........] - ETA: 14s - loss: 0.6721[295. 295.   0.   0.   0.]\n",
      "58/82 [====================>.........] - ETA: 14s - loss: 0.6717[300. 300.   0.   0.   0.]\n",
      "59/82 [====================>.........] - ETA: 13s - loss: 0.6712[305. 305.   0.   0.   0.]\n",
      "60/82 [====================>.........] - ETA: 13s - loss: 0.6703[310. 310.   0.   0.   0.]\n",
      "61/82 [=====================>........] - ETA: 12s - loss: 0.6712[315. 315.   0.   0.   0.]\n",
      "62/82 [=====================>........] - ETA: 11s - loss: 0.6728[320. 320.   0.   0.   0.]\n",
      "63/82 [======================>.......] - ETA: 11s - loss: 0.6746[325. 325.   0.   0.   0.]\n",
      "64/82 [======================>.......] - ETA: 10s - loss: 0.6754[330. 330.   0.   0.   0.]\n",
      "65/82 [======================>.......] - ETA: 10s - loss: 0.6749[335.   5.   0.   0.   0.]\n",
      "66/82 [=======================>......] - ETA: 9s - loss: 0.6749 [340.  10.   0.   0.   0.]\n",
      "67/82 [=======================>......] - ETA: 8s - loss: 0.6750[345.  15.   0.   0.   0.]\n",
      "68/82 [=======================>......] - ETA: 8s - loss: 0.6746[350.  20.   0.   0.   0.]\n",
      "69/82 [========================>.....] - ETA: 7s - loss: 0.6735[355.  25.   0.   0.   0.]\n",
      "70/82 [========================>.....] - ETA: 7s - loss: 0.6733[360.  30.   0.   0.   0.]\n",
      "71/82 [========================>.....] - ETA: 6s - loss: 0.6730[365.  35.   0.   0.   0.]\n",
      "72/82 [=========================>....] - ETA: 5s - loss: 0.6733[370.  40.   0.   0.   0.]\n",
      "73/82 [=========================>....] - ETA: 5s - loss: 0.6729[375.  45.   0.   0.   0.]\n",
      "74/82 [==========================>...] - ETA: 4s - loss: 0.6733[380.  50.   0.   0.   0.]\n",
      "75/82 [==========================>...] - ETA: 4s - loss: 0.6733[385.  55.   0.   0.   0.]\n",
      "76/82 [==========================>...] - ETA: 3s - loss: 0.6729[390.  60.   0.   0.   0.]\n",
      "77/82 [===========================>..] - ETA: 2s - loss: 0.6726[395.  65.   0.   0.   0.]\n",
      "78/82 [===========================>..] - ETA: 2s - loss: 0.6721[400.  70.   0.   0.   0.]\n",
      "79/82 [===========================>..] - ETA: 1s - loss: 0.6717[405.  75.   0.   0.   0.]\n",
      "80/82 [============================>.] - ETA: 1s - loss: 0.6714[ 5. 80.  0.  0.  0.]\n",
      "82/82 [==============================] - 50s 592ms/step - loss: 0.6712\n",
      "Epoch 2/3\n",
      "[10. 85.  0.  0.  0.]\n",
      "[10. 85.  5.  5.  0.]\n",
      " 1/82 [..............................] - ETA: 58s - loss: 0.7580[10. 85. 10. 10.  0.]\n",
      " 2/82 [..............................] - ETA: 47s - loss: 0.7426[10. 85. 15. 15.  0.]\n",
      " 3/82 [>.............................] - ETA: 46s - loss: 0.7146[10. 85. 20. 20.  0.]\n",
      " 4/82 [>.............................] - ETA: 46s - loss: 0.7126[10. 85. 25. 25.  0.]\n",
      " 5/82 [>.............................] - ETA: 45s - loss: 0.7133[10. 85. 30. 30.  0.]\n",
      " 6/82 [=>............................] - ETA: 45s - loss: 0.7128[10. 85. 35. 35.  0.]\n",
      " 7/82 [=>............................] - ETA: 44s - loss: 0.7091[10. 85. 40. 40.  0.]\n",
      " 8/82 [=>............................] - ETA: 43s - loss: 0.7065[10. 85. 45. 45.  0.]\n",
      " 9/82 [==>...........................] - ETA: 43s - loss: 0.7008[10. 85. 50. 50.  0.]\n",
      "10/82 [==>...........................] - ETA: 42s - loss: 0.7001[10. 85. 55. 55.  0.]\n",
      "11/82 [===>..........................] - ETA: 42s - loss: 0.6992[10. 85. 60. 60.  0.]\n",
      "12/82 [===>..........................] - ETA: 41s - loss: 0.6975[10. 85. 65. 65.  0.]\n",
      "13/82 [===>..........................] - ETA: 40s - loss: 0.6969[10. 85. 70. 70.  0.]\n",
      "14/82 [====>.........................] - ETA: 40s - loss: 0.6979[10. 85. 75. 75.  0.]\n",
      "15/82 [====>.........................] - ETA: 39s - loss: 0.7009[10. 85. 80. 80.  0.]\n",
      "16/82 [====>.........................] - ETA: 39s - loss: 0.7006[10. 85. 85. 85.  0.]\n",
      "17/82 [=====>........................] - ETA: 38s - loss: 0.7071[10. 85. 90. 90.  0.]\n",
      "18/82 [=====>........................] - ETA: 37s - loss: 0.7099[10. 85. 95. 95.  0.]\n",
      "19/82 [=====>........................] - ETA: 37s - loss: 0.7091[ 10.  85. 100. 100.   0.]\n",
      "20/82 [======>.......................] - ETA: 36s - loss: 0.7113[ 10.  85. 105. 105.   0.]\n",
      "21/82 [======>.......................] - ETA: 36s - loss: 0.7106[ 10.  85. 110. 110.   0.]\n",
      "22/82 [=======>......................] - ETA: 35s - loss: 0.7096[ 10.  85. 115. 115.   0.]\n",
      "23/82 [=======>......................] - ETA: 34s - loss: 0.7111[ 10.  85. 120. 120.   0.]\n",
      "24/82 [=======>......................] - ETA: 34s - loss: 0.7081[ 10.  85. 125. 125.   0.]\n",
      "25/82 [========>.....................] - ETA: 33s - loss: 0.7092[ 10.  85. 130. 130.   0.]\n",
      "26/82 [========>.....................] - ETA: 33s - loss: 0.7086[ 10.  85. 135. 135.   0.]\n",
      "27/82 [========>.....................] - ETA: 32s - loss: 0.7079[ 10.  85. 140. 140.   0.]\n",
      "28/82 [=========>....................] - ETA: 32s - loss: 0.7105[ 10.  85. 145. 145.   0.]\n",
      "29/82 [=========>....................] - ETA: 31s - loss: 0.7084[ 10.  85. 150. 150.   0.]\n",
      "30/82 [=========>....................] - ETA: 30s - loss: 0.7076[ 10.  85. 155. 155.   0.]\n",
      "31/82 [==========>...................] - ETA: 30s - loss: 0.7048[ 10.  85. 160. 160.   0.]\n",
      "32/82 [==========>...................] - ETA: 29s - loss: 0.7045[ 10.  85. 165. 165.   0.]\n",
      "33/82 [===========>..................] - ETA: 29s - loss: 0.7047[ 10.  85. 170. 170.   0.]\n",
      "34/82 [===========>..................] - ETA: 28s - loss: 0.7044[ 10.  85. 175.   5.   0.]\n",
      "35/82 [===========>..................] - ETA: 27s - loss: 0.7058[ 10.  85. 180.  10.   0.]\n",
      "36/82 [============>.................] - ETA: 27s - loss: 0.7082[ 10.  85. 185.  15.   0.]\n",
      "37/82 [============>.................] - ETA: 26s - loss: 0.7111[ 10.  85. 190.  20.   0.]\n",
      "38/82 [============>.................] - ETA: 26s - loss: 0.7139[ 10.  85. 195.  25.   0.]\n",
      "39/82 [=============>................] - ETA: 25s - loss: 0.7130[ 10.  85. 200.  30.   0.]\n",
      "40/82 [=============>................] - ETA: 24s - loss: 0.7104[ 10.  85. 205.  35.   0.]\n",
      "41/82 [==============>...............] - ETA: 24s - loss: 0.7093[ 10.  85. 210.  40.   0.]\n",
      "42/82 [==============>...............] - ETA: 23s - loss: 0.7087[ 10.  85. 215.  45.   0.]\n",
      "43/82 [==============>...............] - ETA: 23s - loss: 0.7079[ 10.  85. 220.  50.   0.]\n",
      "44/82 [===============>..............] - ETA: 22s - loss: 0.7095[ 10.  85. 225.  55.   0.]\n",
      "45/82 [===============>..............] - ETA: 21s - loss: 0.7082[ 10.  85. 230.  60.   0.]\n",
      "46/82 [===============>..............] - ETA: 21s - loss: 0.7078[ 10.  85. 235.  65.   0.]\n",
      "47/82 [================>.............] - ETA: 20s - loss: 0.7082[ 10.  85. 240.  70.   0.]\n",
      "48/82 [================>.............] - ETA: 20s - loss: 0.7097[ 10.  85. 245.  75.   0.]\n",
      "49/82 [================>.............] - ETA: 19s - loss: 0.7107[ 10.  85. 250.  80.   0.]\n",
      "50/82 [=================>............] - ETA: 18s - loss: 0.7098[ 10.  85. 255.  85.   0.]\n",
      "51/82 [=================>............] - ETA: 18s - loss: 0.7083[ 10.  85. 260.  90.   0.]\n",
      "52/82 [==================>...........] - ETA: 17s - loss: 0.7080[ 10.  85. 265.  95.   0.]\n",
      "53/82 [==================>...........] - ETA: 17s - loss: 0.7077[ 10.  85. 270. 100.   0.]\n",
      "54/82 [==================>...........] - ETA: 16s - loss: 0.7071[ 10.  85. 275. 105.   0.]\n",
      "82/82 [==============================] - 33s 403ms/step - loss: 0.7044\n",
      "Epoch 3/3\n",
      "[ 10.  85.   5. 110.   0.]\n",
      "[ 15.  85.   5. 110.   5.]\n",
      " 1/82 [..............................] - ETA: 59s - loss: 0.6865[ 20.  85.   5. 110.  10.]\n",
      " 2/82 [..............................] - ETA: 47s - loss: 0.7269[ 25.  85.   5. 110.  15.]\n",
      " 3/82 [>.............................] - ETA: 46s - loss: 0.6977[ 30.  85.   5. 110.  20.]\n",
      " 4/82 [>.............................] - ETA: 46s - loss: 0.6952[ 35.  85.   5. 110.  25.]\n",
      " 5/82 [>.............................] - ETA: 45s - loss: 0.7080[ 40.  85.   5. 110.  30.]\n",
      " 6/82 [=>............................] - ETA: 44s - loss: 0.7059[ 45.  85.   5. 110.  35.]\n",
      " 7/82 [=>............................] - ETA: 44s - loss: 0.7168[ 50.  85.   5. 110.  40.]\n",
      " 8/82 [=>............................] - ETA: 43s - loss: 0.7173[ 55.  85.   5. 110.  45.]\n",
      " 9/82 [==>...........................] - ETA: 43s - loss: 0.7184[ 60.  85.   5. 110.  50.]\n",
      "10/82 [==>...........................] - ETA: 42s - loss: 0.7216[ 65.  85.   5. 110.  55.]\n",
      "11/82 [===>..........................] - ETA: 42s - loss: 0.7183[ 70.  85.   5. 110.  60.]\n",
      "12/82 [===>..........................] - ETA: 41s - loss: 0.7244[ 75.  85.   5. 110.  65.]\n",
      "13/82 [===>..........................] - ETA: 40s - loss: 0.7276[ 80.  85.   5. 110.  70.]\n",
      "14/82 [====>.........................] - ETA: 40s - loss: 0.7279[ 85.  85.   5. 110.  75.]\n",
      "15/82 [====>.........................] - ETA: 39s - loss: 0.7300[ 90.  85.   5. 110.  80.]\n",
      "16/82 [====>.........................] - ETA: 39s - loss: 0.7293[ 95.  85.   5. 110.  85.]\n",
      "17/82 [=====>........................] - ETA: 38s - loss: 0.7300[100.  85.   5. 110.  90.]\n",
      "18/82 [=====>........................] - ETA: 37s - loss: 0.7261[105.  85.   5. 110.  95.]\n",
      "19/82 [=====>........................] - ETA: 37s - loss: 0.7217[110.  85.   5. 110. 100.]\n",
      "20/82 [======>.......................] - ETA: 36s - loss: 0.7228[115.  85.   5. 110. 105.]\n",
      "21/82 [======>.......................] - ETA: 36s - loss: 0.7199[120.  85.   5. 110. 110.]\n",
      "22/82 [=======>......................] - ETA: 35s - loss: 0.7216[125.  85.   5. 110. 115.]\n",
      "23/82 [=======>......................] - ETA: 34s - loss: 0.7198[130.  85.   5. 110. 120.]\n",
      "24/82 [=======>......................] - ETA: 34s - loss: 0.7186[135.  85.   5. 110. 125.]\n",
      "25/82 [========>.....................] - ETA: 33s - loss: 0.7186[140.  85.   5. 110. 130.]\n",
      "26/82 [========>.....................] - ETA: 33s - loss: 0.7169[145.  85.   5. 110. 135.]\n",
      "27/82 [========>.....................] - ETA: 32s - loss: 0.7176[150.  85.   5. 110. 140.]\n",
      "28/82 [=========>....................] - ETA: 31s - loss: 0.7133[155.  85.   5. 110. 145.]\n",
      "29/82 [=========>....................] - ETA: 31s - loss: 0.7116[160.  85.   5. 110. 150.]\n",
      "30/82 [=========>....................] - ETA: 30s - loss: 0.7124[165.  85.   5. 110. 155.]\n",
      "31/82 [==========>...................] - ETA: 30s - loss: 0.7140[170.  85.   5. 110. 160.]\n",
      "32/82 [==========>...................] - ETA: 29s - loss: 0.7146[175.  85.   5. 110.   5.]\n",
      "33/82 [===========>..................] - ETA: 29s - loss: 0.7141[180.  85.   5. 110.  10.]\n",
      "34/82 [===========>..................] - ETA: 28s - loss: 0.7136[185.  85.   5. 110.  15.]\n",
      "35/82 [===========>..................] - ETA: 27s - loss: 0.7155[190.  85.   5. 110.  20.]\n",
      "36/82 [============>.................] - ETA: 27s - loss: 0.7176[195.  85.   5. 110.  25.]\n",
      "37/82 [============>.................] - ETA: 26s - loss: 0.7170[200.  85.   5. 110.  30.]\n",
      "38/82 [============>.................] - ETA: 26s - loss: 0.7170[205.  85.   5. 110.  35.]\n",
      "39/82 [=============>................] - ETA: 25s - loss: 0.7192[210.  85.   5. 110.  40.]\n",
      "40/82 [=============>................] - ETA: 24s - loss: 0.7187[215.  85.   5. 110.  45.]\n",
      "41/82 [==============>...............] - ETA: 24s - loss: 0.7184[220.  85.   5. 110.  50.]\n",
      "42/82 [==============>...............] - ETA: 23s - loss: 0.7202[225.  85.   5. 110.  55.]\n",
      "43/82 [==============>...............] - ETA: 23s - loss: 0.7202[230.  85.   5. 110.  60.]\n",
      "44/82 [===============>..............] - ETA: 22s - loss: 0.7221[235.  85.   5. 110.  65.]\n",
      "45/82 [===============>..............] - ETA: 21s - loss: 0.7221[240.  85.   5. 110.  70.]\n",
      "46/82 [===============>..............] - ETA: 21s - loss: 0.7219[245.  85.   5. 110.  75.]\n",
      "47/82 [================>.............] - ETA: 20s - loss: 0.7239[250.  85.   5. 110.  80.]\n",
      "48/82 [================>.............] - ETA: 20s - loss: 0.7238[255.  85.   5. 110.  85.]\n",
      "49/82 [================>.............] - ETA: 19s - loss: 0.7232[260.  85.   5. 110.  90.]\n",
      "50/82 [=================>............] - ETA: 18s - loss: 0.7225[265.  85.   5. 110.  95.]\n",
      "51/82 [=================>............] - ETA: 18s - loss: 0.7209[270.  85.   5. 110. 100.]\n",
      "52/82 [==================>...........] - ETA: 17s - loss: 0.7206[275.  85.   5. 110. 105.]\n",
      "53/82 [==================>...........] - ETA: 17s - loss: 0.7198[280.  85.   5. 110. 110.]\n",
      "54/82 [==================>...........] - ETA: 16s - loss: 0.7201[285.  85.   5. 110. 115.]\n",
      "55/82 [===================>..........] - ETA: 16s - loss: 0.7189[290.  85.   5. 110. 120.]\n",
      "56/82 [===================>..........] - ETA: 15s - loss: 0.7188[295.  85.   5. 110. 125.]\n",
      "57/82 [===================>..........] - ETA: 14s - loss: 0.7180[300.  85.   5. 110. 130.]\n",
      "58/82 [====================>.........] - ETA: 14s - loss: 0.7186[305.  85.   5. 110. 135.]\n",
      "59/82 [====================>.........] - ETA: 13s - loss: 0.7178[310.  85.   5. 110. 140.]\n",
      "60/82 [====================>.........] - ETA: 13s - loss: 0.7178[315.  85.   5. 110. 145.]\n",
      "61/82 [=====================>........] - ETA: 12s - loss: 0.7181[320.  85.   5. 110. 150.]\n",
      "62/82 [=====================>........] - ETA: 11s - loss: 0.7195[325.  85.   5. 110. 155.]\n",
      "63/82 [======================>.......] - ETA: 11s - loss: 0.7194[330.  85.   5. 110. 160.]\n",
      "64/82 [======================>.......] - ETA: 10s - loss: 0.7180[335.  85.   5. 110.   5.]\n",
      "65/82 [======================>.......] - ETA: 10s - loss: 0.7170[340.  85.   5. 110.  10.]\n",
      "66/82 [=======================>......] - ETA: 9s - loss: 0.7169 [345.  85.   5. 110.  15.]\n",
      "67/82 [=======================>......] - ETA: 8s - loss: 0.7163[350.  85.   5. 110.  20.]\n",
      "68/82 [=======================>......] - ETA: 8s - loss: 0.7150[355.  85.   5. 110.  25.]\n",
      "69/82 [========================>.....] - ETA: 7s - loss: 0.7148[360.  85.   5. 110.  30.]\n",
      "70/82 [========================>.....] - ETA: 7s - loss: 0.7136[365.  85.   5. 110.  35.]\n",
      "71/82 [========================>.....] - ETA: 6s - loss: 0.7134[370.  85.   5. 110.  40.]\n",
      "72/82 [=========================>....] - ETA: 5s - loss: 0.7129[375.  85.   5. 110.  45.]\n",
      "73/82 [=========================>....] - ETA: 5s - loss: 0.7134[380.  85.   5. 110.  50.]\n",
      "74/82 [==========================>...] - ETA: 4s - loss: 0.7137[385.  85.   5. 110.  55.]\n",
      "75/82 [==========================>...] - ETA: 4s - loss: 0.7134[390.  85.   5. 110.  60.]\n",
      "76/82 [==========================>...] - ETA: 3s - loss: 0.7131[395.  85.   5. 110.  65.]\n",
      "77/82 [===========================>..] - ETA: 2s - loss: 0.7140[400.  85.   5. 110.  70.]\n",
      "78/82 [===========================>..] - ETA: 2s - loss: 0.7135[405.  85.   5. 110.  75.]\n",
      "79/82 [===========================>..] - ETA: 1s - loss: 0.7126[  5.  85.   5. 110.  80.]\n",
      "80/82 [============================>.] - ETA: 1s - loss: 0.7124[ 10.  85.   5. 110.  85.]\n",
      "82/82 [==============================] - 49s 593ms/step - loss: 0.7117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x294ec4be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "temp_model.fit(temp_gen, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./temp_checkpoint/\"\n",
    "temp_model.save(path + \"checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
