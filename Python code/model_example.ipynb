{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### model example of U_Net module\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. `gen_unet`\n",
    "- `gen_unet` function returns the deep learning model of U-Net architecture.\n",
    "\n",
    "- The architecture was referenced by [singing voice separation with deep u-net convolutional networks](https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf).\n",
    "\n",
    "\n",
    "##### `gen_unet` arguments\n",
    "```python\n",
    "gen_unet(input_shape=(960, 832, 2), encode=5)\n",
    "```\n",
    "\n",
    "- The architecture we used, encodes the input image with 2 strides in convolution layer.\n",
    "- The `encode` argument specify how many times to encode.\n",
    "- Since we used 2 strides, the input shape must be the multiples of $2^{\\ \\textbf{encode} \\ + \\ 1}$.\n",
    "- If not, the concate layer will raise error like below.\n",
    "\n",
    "---\n",
    "```python\n",
    ">>> gen_unet(input_shape=(960, 832, 2), encode=5)\n",
    "```\n",
    "```\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[1], line 3\n",
    "      1 from U_Net import gen_unet\n",
    "----> 3 gen_unet(input_shape=(960, 830, 2), encode=5)\n",
    "```\n",
    "$$\\vdots$$\n",
    "```\n",
    "ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 480, 416, 32), (None, 480, 415, 32)]\n",
    "```\n",
    "---\n",
    "\n",
    "- For example, if `encode=5`, the `input_shape` must be multiples of $2^{\\ 5 \\ + \\  1} = 64$.\n",
    "- So we can set the input shapes like `(960, 832, 2)`, `(768, 512)`.\n",
    "\n",
    "$(960 \\ \\div 64 = 15), \\ (832 \\ \\div 64 = 13), \\ (768 \\ \\div 64 = 12), \\ (512 \\ \\div 64 = 8),$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Total params: 52,404,242\n"
     ]
    }
   ],
   "source": [
    "from U_Net import gen_unet\n",
    "\n",
    "model = gen_unet(input_shape=(960, 832, 2), encode=5)\n",
    "\n",
    "print(\"Total params: {:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the architecture details running cell down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "\n",
    "# model.summary()\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. `generators`\n",
    "- Since we convert music files to image-like data, the memory usage may hit the ceiling and cause several issues.\n",
    "- To avoid this, we made generators to save memory usage in every training step.\n",
    "\n",
    "- There are 4 generators, `TrainGenerator`, `ValidGenerator`, `EvalGenerator`, `PredGenerator`. `PredGenerator` is for sake of `convert_pred` function. We'll describe it later.\n",
    "- `TrainGenerator` was inherited by [`keras.utils.Sequence`](https://keras.io/ko/utils/#sequence), which is basic class for Generators.\n",
    "- `ValidGenerator` and `EvalGenerator` were inherited by `TrainGenerator`, so basically `TrainGenerator`, `ValidGenerator` and `EvalGenerator` are same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from U_Net import TrainGenerator\n",
    "\n",
    "train_path = \"../Data/target_dir/train_data/\"\n",
    "\n",
    "train_gen = TrainGenerator(\n",
    "    src_path=train_path, input_pattern=\"merge*\", output_pattern=\"voice*\", \n",
    "    bulk_num=3, sample_dur=2, \n",
    "    max_mem_size=2, restrict_mem=False, \n",
    "    n_fft=1918, win_length=1024, \n",
    "    sample_rate=110_000, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - `TrainGenerator` arguments\n",
    "\n",
    "```python\n",
    "train_gen = TrainGenerator(\n",
    "    src_path=train_path, input_pattern=\"merge*\", output_pattern=\"voice*\", \n",
    "    bulk_num=3,  sample_dur=2, \n",
    "    max_mem_size=2, restrict_mem=False, \n",
    "    n_fft=1918, win_length=1024, \n",
    "    sample_rate=110_000, shuffle=True\n",
    ")\n",
    "```\n",
    "\n",
    "- The `src_path` is a path to import datasets.\n",
    "- `input_pattern` and `output_pattern` specify the pattern of input-output data of model.\n",
    "    - If `input_pattern=merge*`, the input data will be the files which contains `merge` in front of their names.\n",
    "    - Also if `output_pattern=voice*`, the output data will be the files that contains `voice` in front of their names.\n",
    "\n",
    "- `bulk_num` specify how many data to use in 1 epoch.\n",
    "    -  If `bulk_num=3`, model will train 3 musics in parallel at every epoch.\n",
    "- `sample_dur` determine the sample duration to train model in every steps. `sample_dur` can affect the input shape of model.\n",
    "    - If `sample_dur=2.0`, `TrainGenerator` will load only 2 seconds in dataset, and try to train model.\n",
    "    - For example, at first step, `TrainGenerator` loads 0 ~ 2 seconds of data. \n",
    "    - In second step, `TrainGenerator` loads 2 ~ 4 seconds of data and so on.\n",
    "\n",
    "- `max_mem_size` refers the maximum usage (GB) of memory in every steps. \n",
    "- If `restric_mem=True` and exceed `max_mem_size`, raise `MemoryError`. If not, shows `ResourceWarning`\n",
    "\n",
    "- `n_fft` refers the length of the windowed signal after padding with zeros. `n_fft` can affect the input shape of model.\n",
    "- `win_length` refers the length of window in [STFT](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) operation. `win_length` can affect the input shape of model.\n",
    "\n",
    "- `TrainGenerator` will resolve musics with `sample_rate`. \n",
    "- If `shuffle=True`, `TrainGenerator` will shuffle the datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the image below shows how `TrainGenerator` works.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/TrainGenerator.jpg\" width=500px/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TrainGenerator` can show the sample shape of single data, using `.input_shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 832, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_gen.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your own models using `.input_shape` property like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "(None, 960, 832, 2)\n"
     ]
    }
   ],
   "source": [
    "from U_Net import gen_unet\n",
    "\n",
    "model = gen_unet(train_gen.input_shape)\n",
    "\n",
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `None` is a shape for batch size, so you don't need to bother with.\n",
    "\n",
    "Once you made model, we recommand you to set loss function as `mae`. Then you can train your model with `TrainGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 19:21:48.869421: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 129s 1s/step - loss: 0.7784\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 124s 1s/step - loss: 0.7781\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 124s 1s/step - loss: 0.7715\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 125s 1s/step - loss: 0.7675\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 124s 1s/step - loss: 0.7604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2960dee20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "model.fit(x=train_gen, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. `convert_pred`\n",
    "- After trained your model, you can use `convert_pred` function to check the prediction.\n",
    "\n",
    "- `convert_pred` function uses `PredGenerator` to convert the prediction of model into wave file format, `.wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... [|] : [004/004]\t Done\n"
     ]
    }
   ],
   "source": [
    "from U_Net import convert_pred\n",
    "\n",
    "src_path = \"../Data/target_dir/test_data/\"\n",
    "pred_path = \"../Data/less_trained_pred_sample/\"\n",
    "\n",
    "convert_pred(\n",
    "    model=model, src_path=src_path, pred_dir=pred_path, pattern=\"merge*\", \n",
    "    n_fft=1918, win_length=1024, sample_rate=110_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - `convert_pred` arguments\n",
    "- Use `model` to get prediction.\n",
    "- The `src_path` is a path to import datasets.\n",
    "- `pred_dir` is a directory to save model's prediction.\n",
    "- `pattern` refers to which files in `src_path` to predict.\n",
    "    - If `pattern=*.mp3`, `convert_pred` function will use every data in `src_path` includes `.mp3`\n",
    "\n",
    "- `n_fft` refers the length of the windowed signal after padding with zeros.\n",
    "- `win_length` refers the length of window in [STFT](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) operation.\n",
    "- `PredGenerator` will resolve datasets with `sample_rate`. We recommand to use same rates in training.\n",
    "\n",
    "If you done correctly, `pred_dir` may look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```zsh\n",
    "$ cd ../Data/less_trained_pred_sample && tree\n",
    ".\n",
    "├── merge_007.wav\n",
    "├── merge_008.wav\n",
    "├── merge_009.wav\n",
    "└── merge_010.wav\n",
    "\n",
    "1 directory, 4 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we set the `pattern=merge*`, every file in `src_path` with named `merge...` was converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
